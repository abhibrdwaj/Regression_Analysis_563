> #HW01p3_7.R
> if (FALSE)
+ {"
+ Consider the house price data in Table B.4.
+ a. Fit a multiple regression model relating selling price to all nine regressors.
+ b. Test for significance of regression. What conclusions can you draw?
+ c. Use t tests to assess the contribution of each regressor to the model.
+ Discuss your findings.
+ d. What is the contribution of lot size and living space to the model given that all of the other regressors are included?
+ e. Is multicollinearity a potential problem in this model? "}
> 
> # libraries
> library(faraway)
> library(olsrr)

Attaching package: ‘olsrr’

The following object is masked from ‘package:faraway’:

    hsb

The following object is masked from ‘package:datasets’:

    rivers

> library(psych)

Attaching package: ‘psych’

The following object is masked from ‘package:faraway’:

    logit

> # initial setup
> p3_7_data <- read.csv(file="./Documents/Rutgers/RegressionAnalysis/Data/B4.csv",header = TRUE)
> pdf(file="./Documents/Rutgers/RegressionAnalysis/Output/HW02p3_7_out.pdf")
Error in pdf(file = "./Documents/Rutgers/RegressionAnalysis/Output/HW02p3_7_out.pdf") : 
  cannot open file './Documents/Rutgers/RegressionAnalysis/Output/HW02p3_7_out.pdf'
> pdf(file="./Documents/Rutgers/RegressionAnalysis/Output/HW02p3_7_out.pdf")
> p3_7_data
      y     x1  x2     x3    x4  x5 x6 x7 x8 x9
1  29.5 5.0208 1.0 3.5310 1.500 2.0  7  4 62  0
2  27.9 4.5429 1.0 2.2750 1.175 1.0  6  3 40  0
3  25.9 4.5573 1.0 4.0500 1.232 1.0  6  3 54  0
4  29.9 5.0597 1.0 4.4550 1.121 1.0  6  3 42  0
5  29.9 3.8910 1.0 4.4550 0.988 1.0  6  3 56  0
6  30.9 5.8980 1.0 5.8500 1.240 1.0  7  3 51  1
7  28.9 5.6039 1.0 9.5200 1.501 0.0  6  3 32  0
8  35.9 5.8282 1.0 6.4350 1.225 2.0  6  3 32  0
9  31.5 5.3003 1.0 4.9883 1.552 1.0  6  3 30  0
10 31.0 6.2712 1.0 5.5200 0.975 1.0  5  2 30  0
11 30.9 5.9592 1.0 6.6660 1.121 2.0  6  3 32  0
12 30.0 5.0500 1.0 5.0000 1.020 0.0  5  2 46  1
13 36.9 8.2464 1.5 5.1500 1.664 2.0  8  4 50  0
14 41.9 6.6969 1.5 6.9020 1.488 1.5  7  3 22  1
15 40.5 7.7841 1.5 7.1020 1.376 1.0  6  3 17  0
16 43.9 9.0384 1.0 7.8000 1.500 1.5  7  3 23  0
17 37.5 5.9894 1.0 5.5200 1.256 2.0  6  3 40  1
18 37.9 7.5422 1.5 5.0000 1.690 1.0  6  3 22  0
19 44.5 8.7951 1.5 9.8900 1.820 2.0  8  4 50  1
20 37.9 6.0831 1.5 6.7265 1.652 1.0  6  3 44  0
21 38.9 8.3607 1.5 9.1500 1.777 2.0  8  4 48  1
22 36.9 8.1400 1.0 8.0000 1.504 2.0  7  3  3  0
23 45.8 9.1416 1.5 7.3262 1.831 1.5  8  4 31  0
24 25.9 4.9176 1.0 3.4720 0.998 1.0  7  4 42  0
> describe(p3_7_data)
   vars  n  mean    sd median trimmed   mad   min   max range  skew kurtosis   se
y     1 24 34.61  6.00  33.70   34.43  6.23 25.90 45.80 19.90  0.29    -1.24 1.23
x1    2 24  6.40  1.58   5.97    6.36  1.49  3.89  9.14  5.25  0.35    -1.28 0.32
x2    3 24  1.17  0.24   1.00    1.15  0.00  1.00  1.50  0.50  0.66    -1.62 0.05
x3    4 24  6.03  1.96   5.68    5.98  1.82  2.28  9.89  7.62  0.23    -0.74 0.40
x4    5 24  1.38  0.28   1.43    1.38  0.34  0.98  1.83  0.86  0.05    -1.37 0.06
x5    6 24  1.31  0.60   1.00    1.38  0.74  0.00  2.00  2.00 -0.42    -0.56 0.12
x6    7 24  6.50  0.88   6.00    6.50  0.74  5.00  8.00  3.00  0.36    -0.86 0.18
x7    8 24  3.17  0.56   3.00    3.20  0.00  2.00  4.00  2.00  0.05    -0.29 0.12
x8    9 24 37.46 14.04  40.00   38.05 14.83  3.00 62.00 59.00 -0.41    -0.41 2.87
x9   10 24  0.25  0.44   0.00    0.20  0.00  0.00  1.00  1.00  1.08    -0.86 0.09
> # a) multiple regression model
> lmod <- lm (y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = p3_7_data)
> summary(lmod)

Call:
lm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, 
    data = p3_7_data)

Residuals:
   Min     1Q Median     3Q    Max 
-3.720 -1.956 -0.045  1.627  4.253 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept) 14.92765    5.91285   2.525   0.0243 *
x1           1.92472    1.02990   1.869   0.0827 .
x2           7.00053    4.30037   1.628   0.1258  
x3           0.14918    0.49039   0.304   0.7654  
x4           2.72281    4.35955   0.625   0.5423  
x5           2.00668    1.37351   1.461   0.1661  
x6          -0.41012    2.37854  -0.172   0.8656  
x7          -1.40324    3.39554  -0.413   0.6857  
x8          -0.03715    0.06672  -0.557   0.5865  
x9           1.55945    1.93750   0.805   0.4343  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.949 on 14 degrees of freedom
Multiple R-squared:  0.8531,	Adjusted R-squared:  0.7587 
F-statistic: 9.037 on 9 and 14 DF,  p-value: 0.000185

> summary_lmod <- summary(lmod)
> print(summary_lmod)

Call:
lm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, 
    data = p3_7_data)

Residuals:
   Min     1Q Median     3Q    Max 
-3.720 -1.956 -0.045  1.627  4.253 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept) 14.92765    5.91285   2.525   0.0243 *
x1           1.92472    1.02990   1.869   0.0827 .
x2           7.00053    4.30037   1.628   0.1258  
x3           0.14918    0.49039   0.304   0.7654  
x4           2.72281    4.35955   0.625   0.5423  
x5           2.00668    1.37351   1.461   0.1661  
x6          -0.41012    2.37854  -0.172   0.8656  
x7          -1.40324    3.39554  -0.413   0.6857  
x8          -0.03715    0.06672  -0.557   0.5865  
x9           1.55945    1.93750   0.805   0.4343  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.949 on 14 degrees of freedom
Multiple R-squared:  0.8531,	Adjusted R-squared:  0.7587 
F-statistic: 9.037 on 9 and 14 DF,  p-value: 0.000185

> # Extract F-statistic and p-value
> # b) Test for significance of regression
>
> f_statistic <- summary_lmod$fstatistic
> f_statistic
    value     numdf     dendf 
 9.037027  9.000000 14.000000 
> f_value <- f_statistic[1]
> numdf <- f_statistic[2]
> dendf <- f_statistic[3]
> p_value <- pf(f_value, numdf, dendf, lower.tail = FALSE)
> p_value
       value 
0.0001850299 
> 
> # We get p=0.0001850299, from which we can conclude that the null hypothesis for F-test
> # H0: β1 = β2 = .. = β9 = 0), we reject as it is less than the significance level.
> 
> # c) Use t tests to assess the contribution of each regressor to the model.
> coefficients <- summary_lmod$coefficients
> coefficients
               Estimate Std. Error    t value   Pr(>|t|)
(Intercept) 14.92764759  5.9128516  2.5246106 0.02428304
x1           1.92472156  1.0299013  1.8688408 0.08271059
x2           7.00053420  4.3003717  1.6278905 0.12583613
x3           0.14917793  0.4903874  0.3042043 0.76544692
x4           2.72280790  4.3595535  0.6245612 0.54230434
x5           2.00668402  1.3735086  1.4609912 0.16609649
x6          -0.41012376  2.3785444 -0.1724264 0.86557016
x7          -1.40323530  3.3955419 -0.4132581 0.68567761
x8          -0.03714908  0.0667199 -0.5567916 0.58646103
x9           1.55944663  1.9374959  0.8048774 0.43434718
> significant_regressors <- coefficients[coefficients[, 4] < 0.05, ]
> significant_regressors
   Estimate  Std. Error     t value    Pr(>|t|) 
14.92764759  5.91285162  2.52461055  0.02428304 
> # From here we can conclude, only the p-value for β0 is less than 0.05, hence
> # none of the regressors contribute significantly to the model.
> 
> # d) What is the contribution of lot size (x3) and living space(x4)
> coef(summary(lmod))
               Estimate Std. Error    t value   Pr(>|t|)
(Intercept) 14.92764759  5.9128516  2.5246106 0.02428304
x1           1.92472156  1.0299013  1.8688408 0.08271059
x2           7.00053420  4.3003717  1.6278905 0.12583613
x3           0.14917793  0.4903874  0.3042043 0.76544692
x4           2.72280790  4.3595535  0.6245612 0.54230434
x5           2.00668402  1.3735086  1.4609912 0.16609649
x6          -0.41012376  2.3785444 -0.1724264 0.86557016
x7          -1.40323530  3.3955419 -0.4132581 0.68567761
x8          -0.03714908  0.0667199 -0.5567916 0.58646103
x9           1.55944663  1.9374959  0.8048774 0.43434718
> coefficients
               Estimate Std. Error    t value   Pr(>|t|)
(Intercept) 14.92764759  5.9128516  2.5246106 0.02428304
x1           1.92472156  1.0299013  1.8688408 0.08271059
x2           7.00053420  4.3003717  1.6278905 0.12583613
x3           0.14917793  0.4903874  0.3042043 0.76544692
x4           2.72280790  4.3595535  0.6245612 0.54230434
x5           2.00668402  1.3735086  1.4609912 0.16609649
x6          -0.41012376  2.3785444 -0.1724264 0.86557016
x7          -1.40323530  3.3955419 -0.4132581 0.68567761
x8          -0.03714908  0.0667199 -0.5567916 0.58646103
x9           1.55944663  1.9374959  0.8048774 0.43434718
> coefficients[c("x3", "x4"), ]
    Estimate Std. Error   t value  Pr(>|t|)
x3 0.1491779  0.4903874 0.3042043 0.7654469
x4 2.7228079  4.3595535 0.6245612 0.5423043
> # Based on the p-values, x3 and x4 do not appear to be significant regressors to the model.
> # Although x4 is a large positive coefficient, large standard error denotes the estimate maybe inaccurate
> # Similarly for x3, the coefficient is a small positive value, having less significance (p > 0.05).
> 
> # e) Is multicollinearity a potential problem
> vif(lmod)
       x1        x2        x3        x4        x5        x6        x7        x8        x9 
 7.021036  2.835413  2.454907  3.836477  1.823605 11.710101  9.722663  2.320887  1.942494 
> # As we can observe, the multicollinearity is indeed an issue in our model, as 
> # x6, x7 have very high VIFs indicating these predictors are highly correlated with others in the model.
> 
> 
> # plotting the scatter matrix
> # For part d) where we compare x3 and x4's contribution in the model
> pairs( ~ y+x3+x4,data=p3_7_data)
> # For part e) where we observe high VIF values for x6 and x7
> pairs( ~ y+x6+x7,data=p3_7_data)
> # Basic scatter plot with response and all the regressors
> pairs( ~ y+x1+x2+x3+x4+x5+x6+x7+x8+x9,data=p3_7_data)
> dev.off()
null device 
          1 
> 